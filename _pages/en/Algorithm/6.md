---
title: "Understanding Time Complexity and Space Complexity"
permalink: /en/Algorithm/6
lang: en
tags:
- Algorithm
date: "2024-09-12"
description: "Time complexity and space complexity are critical factors for evaluating the performance of algorithms. This post explores different types and methods for calculating complexity, including Big-O notation."
thumbnail: "/assets/img/thumbnail/algorithm.webp"
---

# Understanding Time Complexity and Space Complexity
---

When evaluating the performance of an algorithm, the two most important factors are **Time Complexity** and **Space Complexity**. These concepts are used to assess how fast the algorithm runs and how much memory it uses.
- Time Complexity: A method to analyze the execution time of an algorithm.
- Space Complexity: A way to measure the amount of memory an algorithm uses.

Both time and space complexity are typically expressed using the `Big O` notation. This is because it provides an estimate of the algorithm’s performance in the worst-case scenario, helping us understand how the algorithm will behave under maximum load.

## What is Time Complexity?
---

Time complexity is used to analyze the execution time of an algorithm. Since execution time can vary based on the runtime environment, we evaluate the time complexity based on the number of basic operations performed.

### Examples of Basic Operations:
1. Data Input/Output - copy, move...
2. Arithmetic Operations - add, multiply...
3. Control Operations - if, while...

### Three Cases of Time Complexity

Time complexity is typically expressed in three cases:
1. Best Case
    - Uses Big Omega (Ω) notation
    - In the best scenario, the algorithm takes at least this much time.
2. Worst Case
    - Uses Big O notation
    - In the worst scenario, the algorithm will take no more time than this.
3. Average Case
    - Uses Big Theta (Θ) notation
    - Represents the average time.

While you might think the average case is the most commonly used, in practice, it becomes very difficult to calculate the average case as algorithms get more complex. Hence, we often evaluate an algorithm’s performance using the worst case.

## How to Calculate Time Complexity
---

Time complexity is generally represented using `Big O` notation. 
When the number of operations is expressed as a polynomial, we ignore all terms except the one with the highest degree, and we also ignore the coefficient of the highest-degree term.

For example, if the input size is n, time complexity is expressed as:
- $$ T(n) = n^2 + 2n + 1 = O(n^2) $$: Only the highest-degree term is kept.
- $$ T(n) = 2n = O(n) $$: The coefficient of the highest-degree term is ignored.

Let’s calculate the time complexity of the following algorithm:

```c
int func (int n) {
  int sum = 0;     // 1 assignment operation
  int i = 0;       // 1 assignment operation
   
  for(i=0; i < n; i++) {  // loop runs n+1 times
    sum += i;             // addition operation runs n times
  }
  for(i=0; i < n; i++) {  // loop runs n+1 times
    sum += i;             // addition operation runs n times   
  }
  return sum;       // 1 return operation
}
```

In the above algorithm, the number of operations per step is noted in the comments, and the total number of operations is `4n+5`. 
Hence, the time complexity of the algorithm is:

$$ T(n) = 4n + 5 = O(n) $$

## Time Complexity Notation
---

### 1. O(1) - Constant Time

When an operation is performed in constant time regardless of the input size (n), the time complexity is O(1).

```c
void func (int n) {
  printf("%d\n", n);
}
```
The above algorithm performs only one operation regardless of n, so the time complexity is:

$$ T(n) = O(1) $$

### 2. O(logN) - Logarithmic Time

If the number of operations increases in proportion to (logN) as the input size (N) grows, the time complexity is O(logN).

```c
for(i=1; i<=n; i*2) {
  ...
}
```

In this algorithm, the value of i doubles each iteration. If this process repeats k times, it results in $$2^k$$, and the loop terminates when:

$$ 2^k = N $$

Taking the logarithm on both sides, we get:

$$ log_2(2^k) = log_2N $$

$$ k = log_2N $$

Since k represents the number of operations, the time complexity is:

$$ T(n) = O(logN) $$

### 3. O(n) - Linear Time

If the number of operations increases linearly with the input size(n), the time complexity is O(n).

```c
for(i=0; i < n; i++) {
  ...
}
```

In this algorithm, the loop iterates n times.
Since the number of operations increases linearly with the value of n, the time complexity is as follows:

$$ T(n) = O(n) $$

### 4. $$O(n^2)$$ - Quadratic Time

If the number of operations increases proportionally to $$n^2$$ as the input size(n) grows, the time complexity is $$O(n^2) $$.

```c
for(i=0; i < n; i++) {
  for(j=0, j < n; j++) {
    ...
  }
}
```
In the algorithm above, since the for-loops are nested, the number of operations increases proportionally to $$n^2$$
The time complexity is as follows:

$$ T(n) = O(n^2) $$

### 5. $$O(2^n)$$ - Exponential Time

If the number of operations grows exponentially with $$2^n$$ as the input size increases, the time complexity is $$O(2^n)$$.

```c
int func (int n) {
  if (n <= 1) 
    return n;
  return func(n-1) + fun(n-2);
}
```

The algorithm above is used to compute the Fibonacci sequence.
Each time the function is called, it recursively calls itself twice, resulting in an increase in the number of operations proportional to $$2^n$$.
Thus, the time complexity is:

$$ T(n) = O(2^n) $$

## Algorithm Performance Comparison
---

Here is a graph comparing the performance of algorithms expressed using Big O notation:

!["Big-O complexity chart"](/assets/img/posts/Algorithm/6/1.webp "Big-O complexity chart"){:class="img-lg"}

$$ O(1) < O(logn) < O(n) < O(nlogn) < O(n^2) < O(2^n) < O(n!) $$

As you move to the right, the time complexity (and thus the execution time) of the algorithms increases.

When the value of n is small, the differences between algorithms may not be significant, but as n grows, the execution time of more complex algorithms increases rapidly.

Reducing time complexity can lead to significant performance improvements in programs.

## Space Complexity
---

Space complexity represents the amount of memory used by an algorithm.

Space complexity is a comprehensive concept that includes both auxiliary space and input space.
Auxiliary space refers to the temporary space used by an algorithm during execution, excluding the input size.

## Calculating Space Complexity
---

Space complexity is also expressed using Big O notation, similar to time complexity.

Here is an example algorithm:

```c
int sum(int a[], int n)
{
  int x = 0;		
  for(int i = 0; i < n; i++) {
    x  = x + a[i];
  }
  return(x);
}
```

The algorithm uses four variables:
- `int arr[n]` : 4*n bytes (input space)
- `int n` : 4 bytes (input space)
- `int x` : 4 bytes (auxiliary space)
- `int i` : 4 bytes (auxiliary space)

The total memory required is `4n + 12` bytes. Since the memory usage grows linearly with the input size, the space complexity is O(n).
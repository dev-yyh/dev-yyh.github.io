---
title: "Understanding asymptotic notation"
permalink: /en/Algorithm/5
lang: en
tags:
    - Algorithm
date: "2024-09-08"
thumbnail: "/assets/img/thumbnail/algorithm.webp"
---

# Asymptotic notation
---

`Asymptotic` refers to the behavior when a function approaches a specific value or boundary when the input size of the function increases infinitely.

`Asymptotic notation` is a way to express the performance of an algorithm by simplifying a complex formula. Rather than comparing exact execution times because execution times vary depending on the environment (e.g. hardware, operating system, language, etc.), we measure efficiency based on the number of basic operations an algorithm performs.

The number of operations an algorithm performs is expressed as a function of its input size n. for example: 

$$ T(n) = n^2 + 3n + 1 $$

The more complex the function, the more difficult it is to compare which algorithm is efficient, so to simplify the function we can use asymptotic notation and express its time and space complexity.

## Why use asymptotic notation?
---

For example, if the time complexity of $$ T(n) = n^2+3n+1 $$ is divided into $$ n^2 $$ and $$3n+1$$ and graphed as follows:

!["Asymptotic notation)"](/assets/img/posts/Algorithm/5/1.webp "Asymptotic notation"){:class="img-lg"}

As the value of $$ n $$ increases, the effect of the value of $$ 3n+1 $$ on the overall time complexity becomes minimal.

In other words, as the value of $$n$$ increases, values ​​below the highest order term do not have a significant effect on the function growth rate.

Since the goal of asymptotic notation is to compare the function with other functions according to the growth rate, all terms except the highest order term and the coefficients of the highest order term are ignored.

Let's consider two algorithms:
- **Algorithm 1**: $$ T(n) = 10000n + 20000 $$
- **Algorithm 2**: $$ T(n) = n^2 + 3n + 1 $$

At first it is difficult to determine which algorithm is more efficient, but it becomes clear when we simplify it using asymptotic notation:

- **Algorithm 1**: $$ T(n) = O(n) $$
- **Algorithm 2**: $$ T(n) = O(n^2) $$

Now it's easy to see that **Algorithm 1** is more efficient. Because the growth rate is slower.

## Three main types of asymptotic notation
---

We use three main asymptotic notations to express the time complexity of our algorithm.

### 1. Big-O notation

Big-O notation describes the **upper bound** of complexity. In other words, it represents how the algorithm behaves in the **worst case**.

Big O notation is expressed as follows:

$$ f(n) = O(g(n)) $$

At this time, $$g(n)$$ is called the `asymptotic upper bound` of the function $$f(n)$$.

This means that the complexity of $$f(n)$$ is less than or equal to $$g(n)$$ even in the worst case. ($$f(n) ≤ g(n)$$)

![Big-O graph](/assets/img/posts/Algorithm/5/2.webp "Big-O graph"){:class="img-lg"}

Big-O notation can be defined as:

$$ O(g(n)) = \{ f(n): \exists c, n_0 \text{ such that } 0 \leq f(n) \leq c \cdot g(n) \text{ for all } n \geq n_0 \} $$

### 2. Big-Ω notation

Big Ome notation represents the **lower bound** of complexity. In other words, it shows how fast the algorithm can run in the **best case**.

The Big Omega notation reads as follows:

$$ f(n) = \Omega(g(n)) $$

At this time, $$g(n)$$ is called the `asymptotic lower bound` of the function $$f(n)$$.

This means that the complexity of $$f(n)$$ is greater than or equal to $$g(n)$$ even in the best case. ($$f(n) ≥ g(n)$$)

![Big Omega graph](/assets/img/posts/Algorithm/5/3.webp "Big Omega graph"){:class="img-lg"}

This can be expressed in a formula as follows:

$$ \Omega(g(n)) = \{ f(n): \exists c, n_0 \text{ such that } 0 \leq c \cdot g(n) \leq f(n) \text{ for all } n \geq n_0 \} $$

### 3. Big-Θ notation

Big theta notation describes cases where the performance of an algorithm falls within both the **upper and lower bounds**. This means that the performance of the algorithm remains constant within the upper and lower bounds.

The big theta notation is written as:

$$ f(n) = \Theta(g(n)) $$

At this time, $$g(n)$$ is called the `asymptotic tight bounds` of the function $$f(n)$$.

This means that the complexity of $$f(n)$$ is within the range of $$g(n)$$ in both the best and worst cases. (approximately $$f(n) = g(n)$$)

![Big Theta graph](/assets/img/posts/Algorithm/5/4.webp "Big Theta graph"){:class="img-lg"}

This can be expressed in a formula as follows:

$$ \Theta(g(n)) = \{ f(n): \exists c_1, c_2, n_0 \text{ such that } 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \text{ for all } n \geq n_0 \} $$

## conclusion
---

In algorithm analysis, understanding **asymptotic notation** plays a very important role in evaluating the efficiency of an algorithm for large inputs.
Knowing the worst case (Big O), best case (Big Ω), and average performance (Big Θ) simplifies and easily compares the performance of algorithms.